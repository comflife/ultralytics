{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6c883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing SiameseYOLOv8s with weights: yolov8s.pt\n",
      "Successfully extracted backbone and head from yolov8s.pt. NC=80, reg_max=16\n",
      "Detected backbone output channels: 32\n",
      "모델 로드 성공! 에폭: 19, 손실: 1.2537\n",
      "모델이 원시 탐지 출력을 반환했습니다. 후처리가 필요합니다.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1, 160, 160] at index 1 does not match the shape of the indexed tensor [1, 64, 160, 160] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_768055/2736705119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# 추론 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwide_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnarrow_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_768055/2736705119.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(model, wide_img_path, narrow_img_path, conf_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;31m# 결과 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mwide_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualize_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwide_img_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# 결과 플롯\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_768055/2736705119.py\u001b[0m in \u001b[0;36mvisualize_predictions\u001b[0;34m(img, predictions, names, conf_threshold)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# 신뢰도 기준으로 필터링\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mconf_threshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [1, 160, 160] at index 1 does not match the shape of the indexed tensor [1, 64, 160, 160] at index 1"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# 시스템 경로에 ultralytics 폴더 추가 (모듈을 찾을 수 있도록)\n",
    "sys.path.append('/home/byounggun/ultralytics')\n",
    "\n",
    "# 모델 및 유틸리티 임포트\n",
    "from yolov8_siamese import SiameseYOLOv8s\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델 로드 함수\n",
    "def load_siamese_model(weights_path, siamese_weights_path):\n",
    "    \"\"\"\n",
    "    학습된 Siamese YOLOv8 모델을 로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        weights_path (str): YOLOv8 베이스 가중치 경로 (보통 'yolov8s.pt')\n",
    "        siamese_weights_path (str): 학습된 Siamese 모델 가중치 경로\n",
    "    \n",
    "    Returns:\n",
    "        model: 로드된 Siamese YOLOv8 모델\n",
    "    \"\"\"\n",
    "    # 먼저 기본 구조 초기화\n",
    "    model = SiameseYOLOv8s(yolo_weights_path=weights_path)\n",
    "    \n",
    "    # 학습된 가중치 로드\n",
    "    ckpt = torch.load(siamese_weights_path, map_location=device)\n",
    "    \n",
    "    # train_siamese.py에서 저장한 체크포인트 구조에 맞게 로드\n",
    "    if 'model_state_dict' in ckpt:\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        print(f\"모델 로드 성공! 에폭: {ckpt['epoch']}, 손실: {ckpt['loss']:.4f}\")\n",
    "    else:\n",
    "        raise ValueError(\"체크포인트 파일 형식이 예상과 다릅니다. 'model_state_dict' 키가 없습니다.\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 이미지 로드 및 전처리 함수\n",
    "def load_and_preprocess_image(image_path, img_size=640):\n",
    "    \"\"\"\n",
    "    이미지를 로드하고 전처리합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일 경로\n",
    "        img_size (int): 이미지 크기 조정 값\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (원본 이미지, 전처리된 텐서)\n",
    "    \"\"\"\n",
    "    # 이미지 로드\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 이미지 크기 및 비율 계산\n",
    "    h, w = img.shape[:2]\n",
    "    ratio = img_size / max(h, w)\n",
    "    if ratio != 1:\n",
    "        interp = cv2.INTER_LINEAR if ratio > 1 else cv2.INTER_AREA\n",
    "        img_resized = cv2.resize(img, (int(w * ratio), int(h * ratio)), interpolation=interp)\n",
    "    else:\n",
    "        img_resized = img\n",
    "    \n",
    "    # 패딩 추가\n",
    "    padded_h, padded_w = int(round(h * ratio)), int(round(w * ratio))\n",
    "    padding = [(img_size - padded_h) // 2, (img_size - padded_w) // 2]\n",
    "    img_padded = np.full((img_size, img_size, 3), 114, dtype=np.uint8)\n",
    "    img_padded[padding[0]:padding[0] + padded_h, padding[1]:padding[1] + padded_w] = img_resized\n",
    "    \n",
    "    # 텐서로 변환 및 정규화\n",
    "    img_tensor = torch.from_numpy(img_padded).permute(2, 0, 1).float().div(255.0).unsqueeze(0)\n",
    "    \n",
    "    return img, img_tensor\n",
    "\n",
    "# 예측 결과 시각화 함수\n",
    "def visualize_predictions(img, predictions, names, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    객체 감지 예측 결과를 시각화합니다.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy.ndarray): 원본 이미지\n",
    "        predictions (torch.Tensor): 모델의 예측 결과\n",
    "        names (list): 클래스 이름 목록\n",
    "        conf_threshold (float): 신뢰도 임계값\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: 바운딩 박스가 그려진 이미지\n",
    "    \"\"\"\n",
    "    img_with_boxes = img.copy()\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Annotator 초기화\n",
    "    annotator = Annotator(img_with_boxes, line_width=2)\n",
    "    \n",
    "    if predictions.shape[0] > 0:\n",
    "        # 신뢰도 기준으로 필터링\n",
    "        predictions = predictions[predictions[:, 4] > conf_threshold]\n",
    "        \n",
    "        for det in predictions:\n",
    "            x1, y1, x2, y2, conf, cls = det[:6]\n",
    "            c = int(cls)\n",
    "            label = f'{names[c]} {conf:.2f}'\n",
    "            \n",
    "            # 바운딩 박스 그리기\n",
    "            annotator.box_label([x1, y1, x2, y2], label, color=colors(c, True))\n",
    "    \n",
    "    return annotator.result()\n",
    "\n",
    "# 시암 특징 유사도 시각화 함수\n",
    "def visualize_similarity(siamese_emb_wide, siamese_emb_narrow):\n",
    "    \"\"\"\n",
    "    시암 네트워크의 특징 유사도를 시각화합니다.\n",
    "    \n",
    "    Args:\n",
    "        siamese_emb_wide (torch.Tensor): 광각 이미지의 특징 임베딩\n",
    "        siamese_emb_narrow (torch.Tensor): 협각 이미지의 특징 임베딩\n",
    "    \n",
    "    Returns:\n",
    "        float: 코사인 유사도 점수\n",
    "    \"\"\"\n",
    "    # 코사인 유사도 계산\n",
    "    similarity = F.cosine_similarity(siamese_emb_wide, siamese_emb_narrow).item()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# 실행 함수\n",
    "# 실행 함수\n",
    "def run_inference(model, wide_img_path, narrow_img_path, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Siamese YOLOv8 모델로 추론을 실행합니다.\n",
    "    \"\"\"\n",
    "    # 이미지 로드 및 전처리\n",
    "    wide_img_orig, wide_img_tensor = load_and_preprocess_image(wide_img_path)\n",
    "    narrow_img_orig, narrow_img_tensor = load_and_preprocess_image(narrow_img_path)\n",
    "    \n",
    "    # 모델을 확실히 평가 모드로 설정\n",
    "    model.eval()\n",
    "    model.training = False\n",
    "    \n",
    "    # 모델 추론 실행\n",
    "    with torch.no_grad():\n",
    "        wide_img_tensor = wide_img_tensor.to(device)\n",
    "        narrow_img_tensor = narrow_img_tensor.to(device)\n",
    "        \n",
    "        # 시암 네트워크의 forward 메소드에 두 이미지 전달\n",
    "        feat_wide_tuple = model.shared_backbone(wide_img_tensor)\n",
    "        feat_narrow_tuple = model.shared_backbone(narrow_img_tensor)\n",
    "        \n",
    "        # 시암 특징 추출\n",
    "        siamese_emb_wide = model._extract_siamese_features(feat_wide_tuple)\n",
    "        siamese_emb_narrow = model._extract_siamese_features(feat_narrow_tuple)\n",
    "        \n",
    "        # 객체 감지 예측\n",
    "        detection_output = model.detection_head(feat_wide_tuple)\n",
    "        \n",
    "        # 출력 형식 확인 및 디버깅\n",
    "        print(f\"Detection output type: {type(detection_output)}\")\n",
    "        if isinstance(detection_output, (list, tuple)):\n",
    "            print(f\"Output is a {type(detection_output).__name__} of length {len(detection_output)}\")\n",
    "            for i, item in enumerate(detection_output):\n",
    "                if torch.is_tensor(item):\n",
    "                    print(f\"Item {i} is a tensor with shape {item.shape} and dtype {item.dtype}\")\n",
    "                else:\n",
    "                    print(f\"Item {i} is a {type(item).__name__}\")\n",
    "        elif torch.is_tensor(detection_output):\n",
    "            print(f\"Output is a tensor with shape {detection_output.shape} and dtype {detection_output.dtype}\")\n",
    "        \n",
    "        # 코사인 유사도 계산\n",
    "        similarity = torch.nn.functional.cosine_similarity(\n",
    "            siamese_emb_wide, siamese_emb_narrow\n",
    "        ).item()\n",
    "    \n",
    "    # 이미지만 표시 (객체 감지 결과 없이)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(wide_img_orig)\n",
    "    plt.title(\"광각 이미지 (원본)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(narrow_img_orig)\n",
    "    plt.title(\"협각 이미지 (원본)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"광각-협각 이미지 코사인 유사도: {similarity:.4f}\")\n",
    "    \n",
    "    return detection_output, similarity\n",
    "\n",
    "# 모델 로드 및 추론 실행 예시\n",
    "# 아래 경로를 실제 경로로 수정하세요\n",
    "if __name__ == \"__main__\":\n",
    "    # 경로 설정\n",
    "    yolo_base_weights = \"yolov8s.pt\"  # YOLOv8 기본 가중치\n",
    "    siamese_model_weights = \"/home/byounggun/ultralytics/runs/my_siamese_training/experiment1/best.pt\"  # 학습된 시암 모델 가중치\n",
    "    \n",
    "    # 테스트 이미지 경로\n",
    "    wide_img_path = \"/home/byounggun/ultralytics/traffics/wide_images_only/train/images/wide_t1_001.jpg\"\n",
    "    narrow_img_path = \"/home/byounggun/ultralytics/traffics/narrow_images_only/train/images/narrow_t1_001.jpg\"\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = load_siamese_model(yolo_base_weights, siamese_model_weights)\n",
    "    \n",
    "    # 추론 실행\n",
    "    predictions, similarity = run_inference(model, wide_img_path, narrow_img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
